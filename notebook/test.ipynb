{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jerome.zhou/anaconda3/envs/predict_py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionEmbedding(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PositionEmbedding, self).__init__()\n",
    "        self.fc_dist = nn.Linear(input_dim-1, output_dim)  # for distance\n",
    "        self.fc_angle = nn.Linear(1, output_dim)  # for angle\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute pairwise distance matrix\n",
    "        x1 = x[:, :2].unsqueeze(1).repeat(1, x.size(0), 1)\n",
    "        x2 = x[:, :2].unsqueeze(0)\n",
    "        dist = torch.sqrt(((x1 - x2) ** 2).sum(-1))\n",
    "\n",
    "        # Compute pairwise angle difference\n",
    "        angle1 = x[:, 2].unsqueeze(1).repeat(1, x.size(0))\n",
    "        angle2 = x[:, 2].unsqueeze(0)\n",
    "        angle_diff = angle1 - angle2\n",
    "\n",
    "        # Pass through fully connected layers\n",
    "        # out_dist = self.fc_dist(dist.unsqueeze(-1))\n",
    "        # out_angle = self.fc_angle(angle_diff.unsqueeze(-1))\n",
    "\n",
    "        # Concatenate distance and angle embeddings\n",
    "        # out = torch.cat([out_dist, out_angle], dim=-1)\n",
    "\n",
    "        return dist, angle_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5500, -0.9624, -1.5492],\n",
      "        [-0.5705,  0.6033, -0.3426],\n",
      "        [-0.1369, -1.2059, -1.2736],\n",
      "        [-1.7726,  0.3708, -0.2174]])\n",
      "(tensor([[0.0000, 2.6359, 1.7043, 3.5802],\n",
      "        [2.6359, 0.0000, 1.8605, 1.2245],\n",
      "        [1.7043, 1.8605, 0.0000, 2.2720],\n",
      "        [3.5802, 1.2245, 2.2720, 0.0000]]), tensor([[ 0.0000, -1.2066, -0.2756, -1.3318],\n",
      "        [ 1.2066,  0.0000,  0.9310, -0.1252],\n",
      "        [ 0.2756, -0.9310,  0.0000, -1.0562],\n",
      "        [ 1.3318,  0.1252,  1.0562,  0.0000]]))\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "A = 4\n",
    "input_dim = 3\n",
    "output_dim = 1\n",
    "model = PositionEmbedding(input_dim, output_dim)\n",
    "\n",
    "# Create a tensor of shape (A, 2)\n",
    "x = torch.randn(A, input_dim)\n",
    "\n",
    "# Forward pass\n",
    "out = model(x)\n",
    "\n",
    "# Print the output\n",
    "print(x)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40789167679667115"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.sqrt((1.1384 - 0.7685)**2 + (1.3972-1.2253)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "from typing import Optional, List\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "\n",
    "\n",
    "def _scaled_dot_product_attention(q, k, v, attn_mask=None, dropout=0.0):\n",
    "    # q           (B * nhead, tgt_len, head_dim)    \n",
    "    # kv          (B * nhead, src_len, head_dim)    \n",
    "    # attn_mask   (B * nhead, 1 or tgt_len, src_len)\n",
    "    # out         (B * nhead, tgt_len, head_dim)\n",
    "\n",
    "    B, Nt, E = q.shape\n",
    "    q = q / math.sqrt(E)\n",
    "\n",
    "    # (B, Nt, E) x (B, E, Ns) -> (B, Nt, Ns)\n",
    "    attn = torch.bmm(q, k.transpose(-2, -1))\n",
    "\n",
    "    # attn mask will set -inf to attn positions that must be masked\n",
    "    # mask is 0 by default so no masking takes place\n",
    "    if attn_mask is not None:\n",
    "        attn += attn_mask\n",
    "\n",
    "    attn = F.softmax(attn, dim=-1)\n",
    "\n",
    "    if dropout > 0.0:\n",
    "        attn = F.dropout(attn, p=dropout)\n",
    "\n",
    "    # (B, Nt, Ns) x (B, Ns, E) -> (B, Nt, E)\n",
    "    output = torch.bmm(attn, v)\n",
    "\n",
    "    return output, attn\n",
    "\n",
    "\n",
    "def _in_projection_packed(q: torch.Tensor,\n",
    "                          k: torch.Tensor,\n",
    "                          v: torch.Tensor,\n",
    "                          w: torch.Tensor,\n",
    "                          b: Optional[torch.Tensor] = None):\n",
    "    r\"\"\"\n",
    "    Performs the in-projection step of the attention operation, using packed weights.\n",
    "    Output is a triple containing projection tensors for query, key and value.\n",
    "\n",
    "    Args:\n",
    "        q, k, v: query, key and value tensors to be projected. For self-attention,\n",
    "            these are typically the same tensor; for encoder-decoder attention,\n",
    "            k and v are typically the same tensor. (We take advantage of these\n",
    "            identities for performance if they are present.) Regardless, q, k and v\n",
    "            must share a common embedding dimension; otherwise their shapes may vary.\n",
    "        w: projection weights for q, k and v, packed into a single tensor. Weights\n",
    "            are packed along dimension 0, in q, k, v order.\n",
    "        b: optional projection biases for q, k and v, packed into a single tensor\n",
    "            in q, k, v order.\n",
    "\n",
    "    Shape:\n",
    "        Inputs:\n",
    "        - q: :math:`(..., E)` where E is the embedding dimension\n",
    "        - k: :math:`(..., E)` where E is the embedding dimension\n",
    "        - v: :math:`(..., E)` where E is the embedding dimension\n",
    "        - w: :math:`(E * 3, E)` where E is the embedding dimension\n",
    "        - b: :math:`E * 3` where E is the embedding dimension\n",
    "\n",
    "        Output:\n",
    "        - in output list :math:`[q', k', v']`, each output tensor will have the\n",
    "            same shape as the corresponding input tensor.\n",
    "    \"\"\"\n",
    "    E = q.shape[-1]\n",
    "    if k is v:\n",
    "        if q is k:\n",
    "            # self-attention\n",
    "            return F.linear(q, w, b).chunk(3, dim=-1)\n",
    "            # q:        (B, *, in_features)         -> (..., E)\n",
    "            # w:        (out_features, in_features) -> (E * 3, E)\n",
    "            # b:        (out_features)              -> (E * 3)\n",
    "            # lin_out:  (B, *, out_features)        -> (..., E * 3)\n",
    "            # chunk_out:                            -> 3 * (..., E)\n",
    "        else:\n",
    "            # encoder-decoder attention\n",
    "            w_q, w_kv = w.split([E, E * 2])\n",
    "            if b is None:\n",
    "                b_q = b_kv = None\n",
    "            else:\n",
    "                b_q, b_kv = b.split([E, E * 2])\n",
    "            # will concat q_out with k_out v_out\n",
    "            #                            |\n",
    "            #                            V\n",
    "            return (F.linear(q, w_q, b_q), ) + F.linear(k, w_kv, b_kv).chunk(\n",
    "                2, dim=-1)\n",
    "    else:\n",
    "        w_q, w_k, w_v = w.chunk(3)\n",
    "        if b is None:\n",
    "            b_q = b_k = b_v = None\n",
    "        else:\n",
    "            b_q, b_k, b_v = b.chunk(3)\n",
    "        return F.linear(q, w_q, b_q), F.linear(k, w_k,\n",
    "                                               b_k), F.linear(v, w_v, b_v)\n",
    "\n",
    "\n",
    "class myMultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, nhead, dropout=0.0, batch_first=False, bias=True):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.dropout = dropout\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        self.head_dim = d_model // nhead\n",
    "        assert (self.head_dim * nhead == d_model), \"d_model % nhead != 0\"\n",
    "\n",
    "        self.in_proj_weight = nn.Parameter(torch.empty((3 * d_model, d_model)))\n",
    "        self.register_parameter('q_proj_weight', None)\n",
    "        self.register_parameter('k_proj_weight', None)\n",
    "        self.register_parameter('v_proj_weight', None)\n",
    "        \n",
    "        if bias:\n",
    "            self.in_proj_bias = nn.Parameter(torch.empty(3 * d_model))\n",
    "        else:\n",
    "            self.register_parameter(\"in_proj_bias\", None)\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.in_proj_weight)\n",
    "        if self.in_proj_bias is not None:\n",
    "            nn.init.constant_(self.in_proj_bias, 0.)\n",
    "            nn.init.constant_(self.out_proj.bias, 0.)\n",
    "\n",
    "    def forward(self,\n",
    "                query: torch.Tensor,\n",
    "                key: torch.Tensor,\n",
    "                value: torch.Tensor,\n",
    "                attn_mask: Optional[Tensor] = None,\n",
    "                key_padding_mask: Optional[Tensor] = None):\n",
    "        #                     Enc             Dec tgt         Dec mem\n",
    "        # query, key, value:  [672, 2, 256]   [100, 2, 256]   [100, 2, 256], [672, 2, 256], [672, 2, 256]\n",
    "        # attn_mask:          None            None            None\n",
    "        # key_padding_mask:   [2, 672]        None            [2, 672]\n",
    "        # output:             [672, 2, 256]   [100, 2, 256]   [100, 2, 256]\n",
    "\n",
    "        # key_padding_mask: used to mask out padding positions after the end\n",
    "        #                   of the input sequence. It depends on the longest\n",
    "        #                   sequence in the batch. Shape (B, src seq length)\n",
    "\n",
    "        # attn_mask:        used in decoders to prevent attention to future\n",
    "        #                   positions using a triangle mask.\n",
    "        #                   2D shape: (tgt seq length, src seq length)\n",
    "        #                   3D shape: (B*nhead, tgt seq length, src seq length)\n",
    "\n",
    "        # q:                (tgt seq length, B, C)\n",
    "        # kv:               (src seq length, B, C)\n",
    "        # out:\n",
    "        #   - attn_output           (tgt seq length, B, C)\n",
    "        #   - attn_output_weights   (B, tgt seq length, C)\n",
    "\n",
    "        is_batched = query.dim() == 3\n",
    "        if self.batch_first and is_batched:\n",
    "            query, key, value = [\n",
    "                x.transpose(1, 0) for x in (query, key, value)\n",
    "            ]\n",
    "\n",
    "        tgt_len, batch_size, embed_dim = query.shape\n",
    "        src_len, _, _ = key.shape\n",
    "\n",
    "        assert (embed_dim == self.d_model\n",
    "                ), f\"expected hidden dim = {self.d_model}, but got {embed_dim}\"\n",
    "        assert (\n",
    "            key.shape == value.shape\n",
    "        ), f\"key shape {key.shape} does not match value shape {value.shape}\"\n",
    "\n",
    "        # compute in-projection\n",
    "        q, k, v = _in_projection_packed(query, key, value, self.in_proj_weight,\n",
    "                                        self.in_proj_bias)\n",
    "\n",
    "        # prep attention mask\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dtype == torch.uint8:\n",
    "                attn_mask = attn_mask.to(torch.bool)\n",
    "            assert attn_mask.is_floating_point(\n",
    "            ) or attn_mask.dtype == torch.bool, \"wrong attn_mask type\"\n",
    "\n",
    "            if attn_mask.dim() == 2:\n",
    "                assert (tgt_len,\n",
    "                        src_len) == attn_mask.shape, \"wrong attn_mask shape\"\n",
    "                attn_mask = attn_mask.unsqueeze(0)\n",
    "                # add artificial batch_size=1\n",
    "            elif attn_mask.dim() == 3:\n",
    "                assert (batch_size * self.nhead, tgt_len,\n",
    "                        src_len) == attn_mask.shape, \"wrong attn_mask shape\"\n",
    "            else:\n",
    "                assert False, \"wrong attn_mask shape\"\n",
    "\n",
    "        # prep key padding mask\n",
    "        if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:\n",
    "            key_padding_mask = key_padding_mask.to(torch.bool)\n",
    "\n",
    "        # reshape q, k, v for multihead attention and make em batch first\n",
    "        # q:    (tgt_len, B, C)->(tgt_len, B, nhead * head_dim)->\n",
    "        #       (tgt_len, B * nhead, head_dim)->(B * nhead, tgt_len, head_dim)\n",
    "        q = q.contiguous().view(tgt_len, batch_size * self.nhead,\n",
    "                                self.head_dim).transpose(0, 1)\n",
    "\n",
    "        # kv:   (src_len, B, C)->(src_len, B, nhead * head_dim)->\n",
    "        #       (src_len, B * nhead, head_dim)->(B * nhead, src_len, head_dim)\n",
    "        # .view(-1, ...) lets python compute the first dim based on the other dims specified\n",
    "        k = k.contiguous().view(-1, batch_size * self.nhead,\n",
    "                                self.head_dim).transpose(0, 1)\n",
    "        v = v.contiguous().view(-1, batch_size * self.nhead,\n",
    "                                self.head_dim).transpose(0, 1)\n",
    "\n",
    "        # update source sequence length after adjustments\n",
    "        src_len = k.shape[1]\n",
    "\n",
    "        # merge key padding and attention masks\n",
    "        if key_padding_mask is not None:\n",
    "            assert key_padding_mask.shape == (\n",
    "                batch_size, src_len\n",
    "            ), f\"expecting key_padding_mask shape of {(batch_size, src_len)}, but got {key_padding_mask.shape}\"\n",
    "\n",
    "            key_padding_mask = key_padding_mask.view(batch_size, 1, 1, src_len)\n",
    "            key_padding_mask = key_padding_mask.expand(-1, self.nhead, -1, -1)\n",
    "            # -1 means not changing the size of that dimension\n",
    "            key_padding_mask = key_padding_mask.reshape(\n",
    "                batch_size * self.nhead, 1, src_len)\n",
    "\n",
    "            if attn_mask is None:\n",
    "                attn_mask = key_padding_mask\n",
    "            elif attn_mask.dtype == torch.bool:\n",
    "                attn_mask = attn_mask.logical_or(key_padding_mask)\n",
    "            else:\n",
    "                attn_mask = attn_mask.masked_fill(key_padding_mask,\n",
    "                                                  float(\"-inf\"))\n",
    "\n",
    "        # convert mask to float\n",
    "        if attn_mask is not None and attn_mask.dtype == torch.bool:\n",
    "            new_attn_mask = torch.zeros_like(attn_mask, dtype=q.dtype)\n",
    "            new_attn_mask.masked_fill_(attn_mask, float(\"-inf\"))\n",
    "            attn_mask = new_attn_mask\n",
    "        \n",
    "        if not self.training:\n",
    "            self.dropout = 0.0\n",
    "\n",
    "        # (deep breath) calculate attention and out projection\n",
    "        attn_output, attn_output_weights = _scaled_dot_product_attention(\n",
    "            q, k, v, attn_mask, self.dropout)\n",
    "        #attn_output            [16, 100, 32]\n",
    "        #attn_output_weights    [16, 100, 672]\n",
    "\n",
    "        attn_output = attn_output.transpose(0, 1).contiguous().view(\n",
    "            tgt_len, batch_size, embed_dim)\n",
    "        #attn_output [16, 100, 32]->[100, 16, 32]->[100, 2, 256]\n",
    "\n",
    "        #attn_output            [100, 2, 256]\n",
    "        #self.out_proj.weight   [256, 256]\n",
    "        #self.out_proj.bias     [256]\n",
    "        attn_output = F.linear(attn_output, self.out_proj.weight,\n",
    "                               self.out_proj.bias)\n",
    "        #attn_output [100, 2, 256]\n",
    "        if self.batch_first and is_batched:\n",
    "            return attn_output.transpose(1, 0), attn_output_weights\n",
    "        else:\n",
    "            return attn_output, attn_output_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "\n",
    "# 创建一个随机输入\n",
    "query = torch.randn(10, 32, 128)\n",
    "key = torch.randn(10, 32, 128)\n",
    "value = torch.randn(10, 32, 128)\n",
    "\n",
    "# 创建并运行nn.MultiheadAttention\n",
    "multihead_attn = nn.MultiheadAttention(128, 8, batch_first=True)\n",
    "output1, _ = multihead_attn(query, key, value)\n",
    "\n",
    "# 创建并运行ManualMultiheadAttention\n",
    "manual_multihead_attn = myMultiheadAttention(128, 8, batch_first=True)\n",
    "\n",
    "# 将nn.MultiheadAttention的参数复制到ManualMultiheadAttention\n",
    "manual_multihead_attn.load_state_dict(copy.deepcopy(multihead_attn.state_dict()))\n",
    "\n",
    "output2, _ = manual_multihead_attn(query, key, value)\n",
    "\n",
    "# 检查两个输出是否一致\n",
    "print(torch.allclose(output1, output2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "qlen = 10\n",
    "klen = 10\n",
    "context_position = torch.arange(qlen, dtype=torch.long,\n",
    "                                        )[:, None]\n",
    "memory_position = torch.arange(klen, dtype=torch.long,\n",
    "                                      )[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
       "        [-1,  0,  1,  2,  3,  4,  5,  6,  7,  8],\n",
       "        [-2, -1,  0,  1,  2,  3,  4,  5,  6,  7],\n",
       "        [-3, -2, -1,  0,  1,  2,  3,  4,  5,  6],\n",
       "        [-4, -3, -2, -1,  0,  1,  2,  3,  4,  5],\n",
       "        [-5, -4, -3, -2, -1,  0,  1,  2,  3,  4],\n",
       "        [-6, -5, -4, -3, -2, -1,  0,  1,  2,  3],\n",
       "        [-7, -6, -5, -4, -3, -2, -1,  0,  1,  2],\n",
       "        [-8, -7, -6, -5, -4, -3, -2, -1,  0,  1],\n",
       "        [-9, -8, -7, -6, -5, -4, -3, -2, -1,  0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_position - context_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.layers.relative_position_bias import RelativePositionBias\n",
    "\n",
    "pos_embed = RelativePositionBias(bidirectional=False,\n",
    "                                 num_buckets=20,\n",
    "                                 max_distance=10,\n",
    "                                 n_heads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 10, 11, 12, 13, 14, 15, 16, 16, 17],\n",
       "        [ 1,  0, 10, 11, 12, 13, 14, 15, 16, 16],\n",
       "        [ 2,  1,  0, 10, 11, 12, 13, 14, 15, 16],\n",
       "        [ 3,  2,  1,  0, 10, 11, 12, 13, 14, 15],\n",
       "        [ 4,  3,  2,  1,  0, 10, 11, 12, 13, 14],\n",
       "        [ 5,  4,  3,  2,  1,  0, 10, 11, 12, 13],\n",
       "        [ 6,  5,  4,  3,  2,  1,  0, 10, 11, 12],\n",
       "        [ 7,  6,  5,  4,  3,  2,  1,  0, 10, 11],\n",
       "        [ 7,  7,  6,  5,  4,  3,  2,  1,  0, 10],\n",
       "        [ 8,  7,  7,  6,  5,  4,  3,  2,  1,  0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embed._relative_position_bucket(memory_position - context_position, bidirectional=True,\n",
    "                                    num_buckets=19, max_distance=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3324, -1.4082, -0.0784,  0.9798],\n",
      "         [-1.5731,  2.0689,  0.8974,  1.5193],\n",
      "         [-0.4568,  0.1283,  0.5386, -2.4981],\n",
      "         [-1.6869,  0.1335,  0.8823, -0.2645]],\n",
      "\n",
      "        [[-0.6141,  0.9644, -0.2441, -1.4504],\n",
      "         [ 0.7161, -0.5827,  0.6977,  1.3610],\n",
      "         [ 0.8721, -0.4602,  1.0663,  0.3069],\n",
      "         [-1.1508, -0.7910,  1.0085, -0.4246]]])\n",
      "Max values: tensor([[-0.3324,  2.0689,  0.8974,  1.5193],\n",
      "        [ 0.8721,  0.9644,  1.0663,  1.3610]])\n",
      "Indices of max values: tensor([[0, 1, 1, 1],\n",
      "        [2, 0, 2, 1]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个三维张量，例如，大小为[2, 4, 4]\n",
    "x = torch.randn(2, 4, 4)\n",
    "\n",
    "# 在第二个维度（channels）上应用torch.max()\n",
    "values, indices = torch.max(x, dim=1)\n",
    "print(x)\n",
    "print(\"Max values:\", values)\n",
    "print(\"Indices of max values:\", indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BN implement\n",
    "bn = nn.BatchNorm2d(num_features=3, eps=0, affine=False, track_running_stats=False)\n",
    "\n",
    "x = torch.rand(10, 3, 5, 5)* 10000\n",
    "offical_bn = bn(x)\n",
    "\n",
    "x_1 = x.permute(1,0,2,3).reshape(3,-1) # [c, n*h*w]\n",
    "mu_x = x_1.mean(dim=-1).view(1,3,1,1) # [1,c,1,1]\n",
    "std_x = x_1.std(dim=-1, unbiased=True).view(1,3,1,1)\n",
    "\n",
    "my_bn = (x-mu_x)/std_x # no epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-5.9740e-06)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(offical_bn-my_bn).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 0.7552,  0.8442,  0.1087,  0.4682],\n",
      "         [ 0.2172,  0.7370,  0.9776,  0.8823],\n",
      "         [-0.7359,  0.6933,  0.3336,  0.8447]]]), tensor([[[ 0.3748,  0.9994,  0.5744,  0.6639],\n",
      "         [-0.1345,  0.5654,  0.3115,  0.5989],\n",
      "         [-0.8560, -0.2329,  0.7621,  0.7718]]]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from typing import Tuple\n",
    "\n",
    "def precompute_freqs_cis(dim: int, seq_len: int, theta: float = 10000.0):\n",
    "    # 计算词向量元素两两分组之后，每组元素对应的旋转角度\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "\n",
    "    # 生成 token 序列索引 t = [0, 1,..., seq_len-1]\n",
    "    t = torch.arange(seq_len, device=freqs.device)\n",
    "    # freqs.shape = [seq_len, dim // 2] \n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    # torch.polar的文档, https://pytorch.org/docs/stable/generated/torch.polar.html\n",
    "    # torch.polar输入参数是abs和angle，abs所有值都一样，abs和angle的shape都一样\n",
    "    # torch.polar输入参数是abs和angle，则freqs_cis = abs*(cos(angle) + sin(angle)i)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_cis\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    # xq.shape = [batch_size, seq_len, dim]\n",
    "    # xq_.shape = [batch_size, seq_len, dim // 2, 2] same as 2维情况\n",
    "    xq_ = xq.float().reshape(*xq.shape[:-1], -1, 2)\n",
    "    xk_ = xk.float().reshape(*xk.shape[:-1], -1, 2)\n",
    "    \n",
    "    # 转为复数域,  xq_.shape = [batch_size, seq_len, dim // 2]\n",
    "    xq_ = torch.view_as_complex(xq_)\n",
    "    xk_ = torch.view_as_complex(xk_)\n",
    "    # 应用旋转操作，然后将结果转回实数域\n",
    "    # xq_out.shape = [batch_size, seq_len, dim]\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(2) #从dim=2维度开始拍平\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(2)\n",
    "\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    seq_len,dim=3,4\n",
    "    freqs_cis = precompute_freqs_cis(dim=dim, seq_len=seq_len, theta=10000.0)\n",
    "    xq = torch.rand(1, seq_len, dim)\n",
    "    xk = torch.rand(1, seq_len, dim)\n",
    "    res = apply_rotary_emb(xq, xk, freqs_cis)\n",
    "    # res的shape是1, seq_len, dim\n",
    "    print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predict_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4a263d47b77ac32555f814086733849591fd88fece6a7709957ad496a0d751e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
